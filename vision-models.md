## Vision Models

[![LLM Vision Models](https://i.ytimg.com/vi_webp/FgT_Mk_bakQ/sddefault.webp)](https://youtu.be/FgT_Mk_bakQ)

You'll learn how to use LLMs to interpret images and extract useful information, covering:

- **Setting Up Vision Models**: Integrate vision capabilities with LLMs using APIs like OpenAI's Chat Completion.
- **Sending Image URLs for Analysis**: Pass URLs or base64-encoded images to LLMs for processing.
- **Reading Image Responses**: Get detailed textual descriptions of images, from scenic landscapes to specific objects like cricketers or bank statements.
- **Extracting Data from Images**: Convert extracted image data to various formats like Markdown tables or JSON arrays.
- **Handling Model Hallucinations**: Address inaccuracies in extraction results, understanding how different prompts can affect output quality.
- **Cost Management for Vision Models**: Adjust detail settings (e.g., "detail: low") to balance cost and output precision.

Here are the links used in the video:

- [Jupyter Notebook](https://colab.research.google.com/drive/1bK0b1XMrZWImtw01T1w9NGraDkiVi8mS)
- [OpenAI Chat API Reference](https://platform.openai.com/docs/api-reference/chat/create)
- [OpenAI Vision Guide](https://platform.openai.com/docs/guides/vision)
- [Sample images used](https://drive.google.com/drive/folders/14MFc7XmGIUDU4-vbmF9305c1SSQrM-gR)
