# 2025 10 22 Project 1 - Session 4 TDS Sep 2025

[![2025 10 22 Project 1 - Session 4 TDS Sep 2025](https://i.ytimg.com/vi_webp/DKsLTjU17pg/sddefault.webp)](https://youtu.be/DKsLTjU17pg)

Duration: 0h 59m

Here's an FAQ summary of the live tutorial:

**Q1: My API credits for the LLM script have run out. I was using OpenAI's GPT-3.5 free tier, which initially worked, but now it's out of credits. I tried other modern GPT versions and Hugging Face, but they require payment. I also tried the course's AI Pipe token, but it didn't work properly. What should I do?**

A1: Your issue has been noted. We found in our internal tests that the analysis shouldn't cost much, certainly not over $1. The solution is designed to work within the provided credit limits. For future projects, if you use your own API keys, the responsibility for outages or credit management falls to you. We recommend having a fallback plan (e.g., a second provider) if one key fails.

**Q2: Since my credits have run out, am I still allowed to modify my code to try different solutions?**

A2: Currently, no retries are scoped. However, we haven't released the final marks yet. If we find that a significant number of students (e.g., 90%) faced the same issue, we might adjust the evaluation methodology or grading curve. This is the most complex evaluation we've ever carried out, so adjustments are possible. We will analyze the overall student performance and the "landscape" of the results before making any decisions. We won't be unfairly brutal.

**Q3: Will the evaluation consider both Round 1 and Round 2, or just Round 1?**

A3: Both Round 1 and Round 2 are planned for evaluation. I spoke with Anand this morning, and he intends to carry out evaluations for at least two rounds. I don't have an update on the exact status or if "completed" means both rounds, but two rounds are expected.

**Q4: Will there be specific weightage for Round 1, or will partial marks be given?**

A4: We haven't decided the weightage yet. We need to analyze the overall performance of all students first. This is why we didn't include weightage in the project description â€“ we wanted to see the actual outcomes and determine if our tests were too difficult or needed tweaking. These are questions the course team will resolve to allow for flexibility in grading.

**Q5: My LLM-generated code for Round 1 is causing a CORS issue when trying to fetch data from the provided JSON link (data.scc.gov API). The API's documentation states it doesn't support CORS. I'm worried about my marks, especially for the third task which has a 20% weight. Changes aren't allowed now, right?**

A5: You are correct, changes are generally not allowed at this stage. I've noted your specific issue, including the CORS problem with the LLM-generated code and your concern about the third task's weight. Please make sure you have detailed logs of these errors, and post them on Discourse if you haven't already. We will discuss these issues with Anand. We will also demonstrate a fully working solution that addresses these problems in the next session.

**Q6: My code is working fine and generating GitHub Pages links that I can see on my own repo's Actions tab. However, I haven't seen any GitHub Pages links for the tasks sent by IITM. I also can't programmatically retrieve these links, and even manually, they're not appearing. What should I do?**

A6: When you generate GitHub Pages, your code needs to programmatically send the resulting URL back to our evaluation server within the specified timeframe (usually 10 minutes). This process was demonstrated in the teaching sessions; I recommend reviewing those. If you're going to your repo's Actions tab manually, that's not how the automated evaluation works. I've noted your concern about not seeing the links, even manually. In the next session, we will show our working solution, which should clarify how to handle this.

**Q7: If I fail to send the GitHub Pages link to your evaluation server (or if it doesn't get a 200 OK response), can I make changes to my code?**

A7: No, making changes to your code after the deadline is not allowed. The entire exercise is designed to be automated. Even if you make changes, it won't impact your evaluation because we will assess your project based on the commit made at a specific point in time (the deadline).

**Q8: I encountered an error where the GitHub repository couldn't be created because the description exceeded 350 characters or included control statements. This happened for both LLM Pages and Share Volume. Can I modify my code to sanitize the description (e.g., truncate it)?**

A8: No changes are allowed to your code for evaluation purposes. If you want to make changes for your own learning, that's fine, but for evaluation, it's not permitted. Please post all your detailed error logs on Discourse. We'll review them to determine if this is a widespread issue or specific to your setup. If it's a common problem, it indicates an issue with our testing methodology.

**Q9: There was an outage with the Gemini API today, and my first task failed, showing a 503 Service Unavailable error. My logs for the first task were purged. Will this be accounted for, especially if I used my own Gemini key?**

A9: I've noted your issue regarding the Gemini outage. If you use your _own_ API key, the responsibility for outages generally falls to you, as opposed to using the course-provided AI Pipe key. We recommend building in a fallback plan into your code so if one key or provider fails, it can switch to another. Please post any logs you have on Discourse, and we'll review them.

**Q10: My Round 1 and Round 2 tasks show "pages build and deployment all jobs were cancelled," but my GitHub repos for Share Volume and LLM Pages were still created. What does this mean?**

A10: This issue has been noted, and it's possible it retried and then succeeded in creating the repos. We are currently waiting for all evaluation results to be finalized. Once they are, we will provide you with specific information, including details on your server responses, which will help clarify if everything worked as expected for your submission.

**Q11: For Share Volume, my GitHub Pages was created, but there was a 404 error fetching data. Gemini AI said it was due to a proxy from AI Pipe. What should I do?**

A11: If you're using the Gemini key with AI Pipe, you should check your Gemini key usage in AI Studio for any outages. A 404 error is unusual for an API refusal; typically, you'd see a different error. Please post all logs and error details on Discourse. If you configured and tested Gemini before, that's good.

**Q12: Has the first evaluation request been sent to all students yet, or is it still in progress?**

A12: The evaluations are still in progress. I haven't received an email from Anand with the preliminary results yet. It's taking a while because there are nearly 1000 submissions, potentially leading to 3000 requests, which are being sent in throttled batches (e.g., 10 concurrent requests) to avoid server flags.

**Q13: My logs show "LLM code generation failure," not "evaluation failure." I'm using Gemini API, and my live Hugging Face link works. Is this still an issue on your side? Also, I use Vercel, and its logs vanish after an hour. Is there another way to save them?**

A13: For your LLM code generation failure, please definitely record all those logs. I will discuss this with Anand tomorrow morning to get a clearer picture. It's possible your system failed on its end, and we might resend the request later. Regarding Vercel logs, you can output them to an online database like Sentry (which is free) for temporary storage. This is a good way to record issues. Remember to save all your logs (request, response, GPT interaction) as this is crucial for diagnosing issues.

**Q14: What is the Share Volume task exactly about?**

A14: The Share Volume task involves being assigned a specific company and its symbol. Your code needs to fetch information from an endpoint (e.g., finding the min, max, highest, and lowest values) and potentially fetch commands from an SCC endpoint. We check for the existence and validity of specific files, their JSON content, and whether certain values (like max objects or values) are present.

**Q15: Only Round 1 seems to be processed so far. Will we have a "best of three tasks" for evaluation?**

A15: It's possible that only Round 1 has been processed for some, while others have completed both rounds. The evaluation process is still ongoing. We haven't decided on a "best of three tasks" approach yet. This will depend on the overall performance across all tasks. For example, if many students fail a particular task (e.g., Task 3), we might not even consider it for evaluation. We will make these decisions once all results are in and we've had a chance to analyze them.

**Q16: Can we edit the code base after the deadline?**

A16: The principle is that you are not supposed to make changes after the deadline. Your code will be evaluated based on the commit at the time of the deadline. We haven't made a final decision on whether late changes will incur a penalty or result in complete disregard of your submission. If you choose to edit, it's at your own risk.

**Q17: Will you provide a solution script for the next session?**

A17: Yes, in the next session, we will demonstrate a fully working solution script, and we'll also share our evaluation script. We will explain how it works. Hopefully, by then, you will have received your results and will be able to ask more meaningful questions, allowing us to provide more meaningful answers.
