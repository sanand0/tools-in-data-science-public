# TA Session 27 June : Processing files with Pandas, Open Refine and Week 3 Graded Assignment

[![TA Session 27 June : Processing files with Pandas, Open Refine and Week 3 Graded Assignment](https://i.ytimg.com/vi_webp/D0b4n1k2K5I/sddefault.webp)](https://youtu.be/D0b4n1k2K5I)

Duration: 1h 51m

Here's an FAQ-style summary of the tutorial, incorporating your requirements:

---

### Tools in Data Science (TDS) Live Tutorial: Week 3 Graded Assignment FAQ

**Q1: What's the plan for today's session?**

**A1:** Today, we'll focus on the Week 3 content, primarily the graded assignment. I'll quickly go over some initial topics to ensure everyone is on the same page.

**Q2: I'm having trouble with general concepts like Excel cleanup or text-to-columns. What should I do?**

**A2:** These are very simple and straightforward concepts. If you're struggling, I'd suggest re-watching the videos or asking your friends. It mainly requires practice. Data aggregation, for example, is quite easy once you get the hang of it.

**Q3: How important are the shell command videos for data preparation? Will they be on the exams?**

**A3:** Shell commands _might_ appear in the end-term exam, though it's less likely for the ROE. We're primarily focusing on using Pandas for data preparation this term, as it's simpler for most people and accomplishes the same tasks.

**Q4: What about the data preparation in editor video? Is it useful?**

**A4:** Yes, that video is great! It offers many handy tips and shortcuts to work faster. Honestly, I should probably learn many of those shortcuts myself!

**Q5: I had issues with OpenRefine. Is it supposed to be tricky?**

**A5:** OpenRefine can sometimes be a bit tricky. If you had issues, you're not alone.

**Q6: How do I get the log file into the VS Code explorer sidebar?**

**A6:** There are a couple of ways:

1. **Open Folder:** When you start VS Code, use "Open Folder" and select the directory where your log file is located. This sets your workspace, and the file should then appear in the sidebar.
2. **Terminal Command:** Open your terminal _in_ the folder where the log file is. Then type `code .` (that's `code` followed by a space and a period) and press Enter. This will open VS Code with that folder as the workspace.

**Q7: The log file from the portal is a `.gz` file. How do I open/unzip it on Windows?**

**A7:** You need to decompress the `.gz` file first.

- **Google Colab:** You can upload it to Google Colab and use the `!gunzip -k "filename.gz"` command in a code cell. This will create an unzipped version in your Colab environment, which you can then download.
- **Windows:** You can install a free file archiving software like 7-Zip, which can handle `.gz` files and extract their contents. Alternatively, if Python is installed on your system, you can use Python code to decompress it.

**Q8: When creating a virtual environment in VS Code, which method should I use, and does it require specific permissions?**

**A8:** You can use either the command palette or the terminal for virtual environment creation.

1. **Command Palette:** Press `Ctrl+Shift+P` (Windows) or `Cmd+Shift+P` (Mac/Linux), then search for "Python: Create Environment" and follow the prompts. This method is often preferred as VS Code is a trusted source and usually handles permissions automatically.
2. **Terminal:** Open the terminal in your workspace (`Ctrl+Tilt` on Windows), then type `python -m venv .venv` (where `.venv` is your chosen environment name).

If you encounter a "permissions issue" when trying to activate scripts in the terminal, it's because your system is configured not to run scripts for security reasons. There are online solutions to adjust this, but for creating the virtual environment itself, the command palette method often bypasses this as VS Code is a trusted application.

**Q9: Once I have the unzipped log file and my Jupyter Notebook open, what's the first step to process the data in Pandas?**

**A9:** The log file is a plain text file, not a CSV. So, you can't use `pd.read_csv()`.

1. First, you need to read the text file into your Python environment as a list of strings. You can do this using basic Python file handling:
   ```python
   with open("access_log", "r") as f:
       lines = f.readlines()
   ```
2. Then, to get this into a Pandas DataFrame, you can initially create a DataFrame with just this list:
   ```python
   import pandas as pd

   df = pd.DataFrame(lines)
   ```
   This will put each line into a single column named `0`.

**Q10: The DataFrame has all lines in one column. How do I split them into separate columns based on the log file structure?**

**A10:** The challenge is that the fields aren't separated by a single, consistent delimiter (like commas or tabs) that `split()` can easily handle for all fields. Some fields (like the `user_agent` or `referer`) might contain spaces themselves.

Here's a strategy:

1. **Leverage Knowns:** We know some fields are always at the beginning (IP, user, timestamp, request method/URL/protocol) and some are always at the end (user agent, server). We can use `split()` and `rsplit()` (right-split) to extract these reliably.
2. **Handle the Messy Middle:** The `referer` field is often the most problematic because it can contain spaces, making simple splitting difficult. We can isolate this field as a single column and deal with it separately, or even ignore it if not strictly needed for the questions.

**Q11: Can you demonstrate a clever splitting method using `split()` and `rsplit()`?**

**A11:** Yes. The key is to split from both ends for the 'clean' parts:

Let's say `lines[0]` is your first log entry:

- `lines[0].split(' ', 10)`: This would split from the left, at most 10 times, giving you the first 10 fields (IP, user, timestamp, method, URL, protocol, status, size).
- `lines[0].rsplit(' ', 2)`: This would split from the right, at most 2 times, effectively giving you the main part of the log entry, and separately, the user agent and the server at the very end.

By combining these, you can extract the consistent fields, leaving the "dirty" `referer` field as a single, large string that can be dealt with or ignored.

**Q12: How do I remove the `[` and `]` characters from the timestamp field once it's in a DataFrame?**

**A12:** You can use Pandas string methods for this. For a column named `timestamp`:

```python
df["timestamp"] = df["timestamp"].str.replace("[", "").str.replace("]", "")
```

Make sure to escape the square brackets with a backslash if they are literal characters you want to replace (e.g., `\.str.replace('\[', '')`).

**Q13: How do I convert the cleaned timestamp string column into a proper datetime object for easier filtering?**

**A13:** Use `pd.to_datetime()` and specify the format string:

```python
df["timestamp"] = pd.to_datetime(df["timestamp"], format="%d/%b/%Y:%H:%M:%S %z")
```

- `%d`: Day of the month as a zero-padded decimal number.
- `%b`: Month as localeâ€™s abbreviated name.
- `%Y`: Year with century as a decimal number.
- `%H`: Hour (24-hour clock) as a zero-padded decimal number.
- `%M`: Minute as a zero-padded decimal number.
- `%S`: Second as a zero-padded decimal number.
- `%z`: UTC offset in the form +HHMM or -HHMM.

**Q14: How do I filter the DataFrame to show only entries that occurred on a specific day, like Tuesdays?**

**A14:** Once your timestamp column is a datetime object, you can access date-related attributes.

```python
df_tuesdays = df[df["timestamp"].dt.day_name() == "Tuesday"]
```

**Q15: How can I identify all unique status codes in my dataset and filter for specific ranges, like successful requests (200s)?**

**A15:**

1. **Unique values:** Use `df['status'].unique()` to see all unique status codes.
2. **Filtering for 200s:**
   ```python
   df_successful = df[df["status"] == 200]
   ```
   If you want a range (e.g., 200-299):
   ```python
   df_range = df[(df["status"] >= 200) & (df["status"] < 300)]
   ```

**Q16: How do I convert a column that contains numeric values (like `size`) but is currently an object/string type, into an integer or float type?**

**A16:** Use `pd.to_numeric()`.

```python
df["size"] = pd.to_numeric(df["size"], errors="coerce")
```

The `errors='coerce'` argument will replace any non-numeric values with `NaN` (Not a Number), allowing the conversion to proceed without errors. If you know there might be floating-point numbers, use `float` instead of `int`.

**Q17: How can I save my processed DataFrame to a CSV file?**

**A17:** Use the `to_csv()` method:

```python
df_processed.to_csv("processed_log.csv", index=False)
```

`index=False` prevents Pandas from writing the DataFrame index as a column in the CSV.

**Q18: How do I deal with misspellings or variations in categorical data (like city names) in OpenRefine?**

**A18:** OpenRefine has a powerful feature called "Text Facet".

1. **Text Facet:** Apply a Text Facet to the column with variations (e.g., "city").
2. **Cluster:** Inside the Text Facet, you'll see an option to "Cluster". Click this. OpenRefine uses various algorithms to identify similar-looking entries (e.g., "Dhaka" and "Dhaka ").
3. **Merge:** It will present suggestions for merging. You can then select which variations to merge into a single, standardized value. This speeds up data cleaning significantly. You'll still need some manual discretion, but it's much faster than doing everything manually.

**Q19: How do I save changes I've made in OpenRefine?**

**A19:** In OpenRefine, after making your changes, there's an "Export" button in the top right corner. Click this, and you can choose to export your cleaned data in various formats like CSV.

---
