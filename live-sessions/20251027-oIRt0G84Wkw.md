# 2025 10 15Project 1 - Session 3 TDS Sep 2025

[![2025 10 15Project 1 - Session 3 TDS Sep 2025](https://i.ytimg.com/vi_webp/oIRt0G84Wkw/sddefault.webp)](https://youtu.be/oIRt0G84Wkw)

Duration: 1h 33m

Here's an FAQ summary of the TDS live tutorial Q&A session:

**Q1: I've finished my project and tested it locally. For submission, I need to provide a GitHub repo, API endpoint, and a secret code in the Google Form. What exactly should be in my GitHub repo, what is the secret code, and for the evaluator (instructor) step, will my test code also be required in the repo? I also saw a mention of a "demo trial" after submission, do I need to do anything for that? Finally, where can I find the Google Form?**

**A1:** Your GitHub repo should contain only your student application code. The secret code is the private token you enter in Hugging Face. The test code used for your local testing does not need to be pushed to your GitHub repo, as the evaluation server will generate its own requests for testing. The "demo trial" is for our automated evaluation, and you don't need to do anything. You can find the Google Form on the project page under the "Evaluate" section.

**Q2: My application isn't fully passing all test cases, and I'm currently manually checking the generated JSON output for evaluation. Is this manual checking sufficient? What exactly are "corner cases" that I should focus on? If my app isn't _fully_ functional but some parts work, will I get zero or partial marks? Also, I heard about issues with Gemini API in the last term; what's your recommendation regarding LLMs, and will similar issues affect my project's grading?**

**A2:** Yes, manual checking is acceptable for your testing. "Corner cases" refer to testing all possible conditions and ensuring your application handles all instructions correctly, including authentication, validation, and proper error/exception logging. You will receive partial marks based on the subsections of your submission that pass; it's not an all-or-nothing score. We recommend using AI Pipe for your LLM interactions, as we have control and can support it. Using other LLMs like Gemini is at your own risk because their API rate limits or changes could affect your project's functionality and evaluation, as happened in the previous term.

**Q3: My graded assignment deadlines appear incorrect on the platform. Can you confirm the correct deadlines? Also, my AI Pipe token shows 75% used; when does it refresh? What's the agenda for today's session, and where can I find project-specific learning materials if I missed earlier sessions?**

**A3:** The deadlines are based on your individual time zone, but generally, GA1/GA2 are Oct 5th, GA3 is Oct 12th, P1 is Oct 17th, and GA4 is Oct 19th. Your AI Pipe token refreshes and reconciles automatically once every seven days. Today's session is a query-solving session. For project-specific learning and building the project from scratch, please refer to the YouTube sessions held on October 4th, 5th, and 12th.

**Q4: My project shows "status accepted" but then "command failed" on GitHub Pages. I'm having trouble getting the pages to deploy correctly, and manual branch setting keeps reverting. Can I automate this, and what's the recommended way to quickly deploy Fast APIs for GitHub Pages? Will the evaluation process run Round 1 and Round 2 continuously, or will it reset for each round, potentially creating new repos due to my unique ID generation? What about the attachments (like `data.csv`) mentioned in the template; will they be small enough, and where can I find them?**

**A4:** The "command failed" error indicates your GitHub Pages are not deploying. You should be able to automate GitHub Pages deployment using GitHub Actions (code blocks), as relying on manual settings is not scalable. For quick Fast API deployment, Hugging Face is the recommended platform. The evaluation process will run Round 1 and Round 2 continuously for a _single task_. It will not reset the template or create new repos for the same task. The attachments like `data.csv` are typically small enough for LLM processing, and you can find them within the sample project template. "Corner cases" refer to testing scenarios like authentication errors, validation issues, or unexpected inputs to ensure robust error handling.

**Q5: Is using Flask acceptable, or should I switch to FastAPI? Is it okay if my GitHub repo is hosted on Hugging Face? What's the recommended LLM for this project, and if I face challenges with logging or prompt building, can ChatGPT assist me?**

**A5:** Flask is acceptable for the project. Yes, hosting your GitHub repo and deploying your application on Hugging Face is perfectly fine. We recommend using AI Pipe for your LLM interactions as it's directly supported. For logging and prompt building, ChatGPT can definitely assist you, but remember that prompt engineering is an iterative process requiring trial and error. You can ask it to generate log entries for each step of your application.

**Q6: My LLM is only producing a partial app (e.g., HTML page with markdown output). How can I get a complete functional app? Also, should I build a static or dynamic app? What are "corner cases" for testing?**

**A6:** The issue with partial app generation or markdown output is likely due to your prompting. Focus on building efficient and detailed prompts. You should be building a _static_ app. "Corner cases" refer to testing scenarios like authentication errors, validation issues, or unexpected inputs to ensure robust error handling.

**Q7: Will I get partial marks if my app is not fully functional, but other parts are passing? Also, what happens if I finish my project at the last minute?**

**A7:** Yes, you will get partial marks based on the subsections that pass. While experience helps, it's always recommended to start earlier, especially if you need time to debug and clarify doubts on platforms like Discord. Don't wait until the last minute.

**Q8: For GitHub Pages deployment, my app's URL generation is faulty and requires manual fixes. Will the evaluation process account for this? Also, what are "corner cases" for testing, and what attachments are relevant?**

**A8:** The platform expects programmatic handling for URL generation, not manual intervention. If your app requires manual fixes, it will affect the evaluation. "Corner cases" refer to testing scenarios like authentication errors, validation issues, or unexpected inputs. The `data.csv` file mentioned in the sample project template is an example of an attachment, and it will be small enough for LLM processing.

**Q9: I'm having trouble understanding how to enable GitHub Pages automatically using code. Can you clarify, and will ChatGPT help with this?**

**A9:** You should be able to automate GitHub Pages deployment using GitHub Actions (code blocks) and the GitHub library's `auth` function. It's not a global setting but specific to each repository. ChatGPT can assist with prompt building for this, but it's an iterative process. Please refer to the specific YouTube session (Oct 4th) for detailed guidance.

**Q10: Regarding grading, how will it be? Will it be full score or no score, or will there be partial points for test cases? Also, I am using Flask for my project, is it fine?**

**A10:** Grading will involve partial marks based on subsections that pass; it's not a zero-or-full score. Flask is perfectly acceptable for the project.

**Q11: You mentioned that I'll face challenges with logs. Can ChatGPT help me in fixing those log-related issues?**

**A11:** Yes, ChatGPT can help you build effective prompts for logging, which is crucial for debugging. Prompt building is an iterative process, so be prepared to try, build, and refine your prompts.

**Q12: If I'm building my app, should I create a static or dynamic app? What are "corner cases" for testing, and what attachments are expected?**

**A12:** You should build a _static_ app. "Corner cases" involve testing various conditions like authentication errors, validation, and exception logging. The `data.csv` file in the sample project template is an example of an expected attachment.

**Q13: My app isn't fully functional, but other parts pass. Will I get partial marks? I have experience and plan to finish at the last minute; is that okay?**

**A13:** Yes, you will get partial marks. It's recommended to start earlier to resolve issues and clarify doubts.

**Q14: I've sent you an email with questions, but it seems the email address was wrong. Can you provide the correct email ID for submitting queries?**

**A14:** Please use Discord for queries, as it's the official channel for support.

**Q15: When evaluating my project, it only generates a partial HTML page with markdown output. How can I fix this to get a complete, functional app?**

**A15:** This issue is likely due to your LLM prompts. Focus on creating more efficient and detailed prompts to ensure the complete app is generated.

**Q16: I understand the deadline is October 17th. I'm busy with other courses, but I have experience. Can I realistically complete the project in the last minute?**

**A16:** While your experience helps, it's advisable to start earlier. Don't wait until the last minute, as you'll need time to debug and clarify any doubts. If you encounter issues, raise them on Discord.

**Q17: You mentioned I might face challenges with logs. Will ChatGPT help me resolve these issues?**

**A17:** Yes, ChatGPT can assist with prompt building for effective logging. It's an iterative process, so be prepared to refine your prompts.

**Q18: I'm experiencing an error where my app's URL generation is faulty, requiring manual fixes for my repo to show. What are "corner cases" for testing, and what attachments are expected?**

**A18:** The platform expects programmatic handling for URL generation. Manual fixes are not part of the automated evaluation. "Corner cases" involve testing various conditions, and the `data.csv` file is an example of an expected attachment.

**Q19: What exactly are "corner cases" when testing my application?**

**A19:** Corner cases refer to testing all possible conditions, including authentication errors, validation issues, or unexpected inputs, to ensure robust error handling.

**Q20: What attachments are expected for the project submission?**

**A20:** The `data.csv` file, as shown in the sample project template, is an example of an expected attachment. These attachments will be small enough for LLM processing.

**Q21: My project has an issue with URL generation; it's faulty and requires manual fixes. What are "corner cases" for testing, and what attachments are expected?**

**A21:** The platform expects programmatic handling for URL generation. Manual fixes are not part of the automated evaluation. "Corner cases" involve testing various conditions, and the `data.csv` file is an example of an expected attachment.
