# 2025 10 04 Project 1 - Session 1 TDS Sep 2025

[![2025 10 04 Project 1 - Session 1 TDS Sep 2025](https://i.ytimg.com/vi_webp/UlUf6roSG3c/sddefault.webp)](https://youtu.be/UlUf6roSG3c)

Duration: 2h 24m

Here's an FAQ summary of the live tutorial:

---

## Tools in Data Science (TDS) Tutorial: Project 1 Walkthrough - FAQ

This FAQ covers common questions and answers from a live tutorial session on Project 1 of Tools in Data Science (TDS), focusing on setting up a GitHub-integrated application.

---

**Q1: What is the overall flow of the application for Project 1?**

**A1:** The TDS server sends a request with a `task ID` to your endpoint. Your endpoint needs to:

1.  Generate code for **Round 1**.
2.  Create a GitHub repository.
3.  Push the generated code, including `LICENSE` and `README` files, to this repository.
4.  Enable GitHub Pages for the repository.
5.  Send a response back to the ITM TDS server within 10 minutes.

For **Round 2**, the server sends another request to your endpoint, which should modify the same repository, likely using a tool like GPT.

**Q2: What are the required fields for submission?**

**A2:** You need to provide:

1.  **Deployment URL:** The URL where your application is deployed (e.g., Hugging Face).
2.  **Secret Key:** A secret string (like a password) that you define. We will use this to send requests to your endpoint, and your application should validate it to ensure requests are from an authorized source.
3.  **GitHub Homepage URL:** The URL of the GitHub repository containing your Project 1 code.

**Q3: Does the entire GitHub repository need to be public, or just specific files/folders?**

**A3:** The entire GitHub repository that contains your Project 1 code must be public. This includes the main repository you manage and any new repositories created during the request processing (like for Round 1). This is necessary for the system to access your code and for GitHub Pages to work.

**Q4: Can I keep my GitHub repository private until the submission deadline?**

**A4:** Yes, you can keep your repository private until the deadline. However, you must make it public at least 20 minutes (or whatever time allows) before the deadline to ensure it can be accessed for evaluation.

**Q5: What type of connection should I use for GitHub (SSH or HTTPS)?**

**A5:** For submitting your GitHub homepage URL, HTTPS will work. SSH is typically used for cloning a repository to your local machine and pushing changes, but for the submission itself, HTTPS is fine.

**Q6: What issues have other students faced with LLM integration, specifically with Gemini AI?**

**A6:** Some students have reported encountering 500 internal errors from Google's Gemini AI servers and issues with the context "running out." The instructor suggests using AI Pipe (a provided tool) instead of directly calling Gemini AI.

**Q7: Does AI Pipe have internet capabilities to read raw GitHub content?**

**A7:** The instructor is unsure if AI Pipe can directly read raw GitHub content from a URL. They recommend creating a test GitHub repository, placing some files there, and testing AI Pipe's capabilities to see if it works for your specific use case. While AI Pipe uses Gemini models, its ability to fetch external content directly is not confirmed.

**Q8: How will the system send requests to my endpoint?**

**A8:** The ITM server will send a POST request to your endpoint (the URL you provide) with a JSON payload. This payload will contain specific fields like `email`, `secret`, `task`, `round`, `nonce`, `brief`, `texts`, `evaluation_url`, and `attachments`. Your endpoint needs to be configured to accept and process this POST request.

**Q9: Can I use any name for my endpoint?**

**A9:** Yes, the endpoint name (e.g., `/handle_task`) is arbitrary. You can choose any name, but you must provide the correct URL in the submission form.

**Q10: Why is the `async def` keyword used in the FastAPI endpoint?**

**A10:** The `async def` keyword in FastAPI is used for handling concurrency, allowing the server to process multiple requests simultaneously. However, for Project 1, the system will send single-threaded requests, so you can remove `async` if you prefer, as concurrency is not strictly required for this project.

**Q11: How can I test my endpoint before the official evaluation?**

**A11:** The documentation provides three example JSON payload templates that you can use to send test requests to your endpoint. The instructor also demonstrates sending requests from a separate "instructor" script to simulate the ITM server.

**Q12: When will my application be evaluated?**

**A12:** Evaluation will occur after the submission deadline, likely immediately after, though the exact timing is not specified.

**Q13: How can I verify that my application is correctly receiving and processing the request payload?**

**A13:** The request payload sent by the ITM server includes a `data` field. If your application successfully processes the request and sends the `data` field back in its response, it indicates that your application is able to read the data correctly.

**Q14: What is the purpose of the `nonce` field in the request payload?**

**A14:** The `nonce` (number used once) is a unique token generated using the UUID7 module. It is for the ITM server to maintain and track requests. You don't need to generate it, but your application should be able to receive it. You can potentially use the `nonce` along with the task name to ensure uniqueness if you decide to handle multiple files or refactor code.

**Q15: What specific functions are needed for Round 1?**

**A15:** For Round 1, you will need to implement functions that:

1.  Write code using an LLM (e.g., AI Pipe).
2.  Create a GitHub repository.
3.  Enable GitHub Pages.
4.  Push generated files (like `index.html`) to the repository.

**Q16: Do I need to provide a `README.md` and `LICENSE` file for my repository?**

**A16:** Yes, your repository needs to have `README.md` and `LICENSE` files. If you set `auto_init` to `true` when creating the repository via the GitHub API, these files will be generated automatically with default content. You can later edit these files.

**Q17: What permissions are required for the GitHub Personal Access Token (PAT)?**

**A17:** For the fine-grained token, you need the following permissions set to **write**:

- `contents`
- `workflows`
- `administration`

These permissions allow the system to create the repository, push code, and manage repository settings (like enabling GitHub Pages).

**Q18: What is the expected status code for a successful repository creation via the GitHub API?**

**A18:** A successful repository creation should return a `201` status code. If a different status code is received, it indicates an error (e.g., 401 for unauthorized, 403 for forbidden due to incorrect permissions).

**Q19: How can I get the SHA of the latest commit?**

**A19:** You can use the GitHub API to get the SHA of the latest commit for a specific branch in your repository. This is typically done by making an API call to retrieve the commit history. This SHA is required when updating an existing file in Round 2.

**Q20: What is the purpose of Base64 encoding for file content?**

**A20:** When pushing file content via the GitHub API, you cannot directly send the file as plain text. The content must be Base64 encoded. This converts the file's binary data into a string format that can be safely transmitted within a JSON payload.

**Q21: How does the system handle updating files in Round 2?**

**A21:** For Round 2, if you are updating an existing file, you must provide its `SHA` (Secure Hash Algorithm) in the API request payload. This SHA acts as an identifier for the specific version of the file you are updating. The `get_sha_of_latest_commit` function is used to retrieve this SHA.

**Q22: Is the `index.html` file hardcoded, or will it be generated by LLM?**

**A22:** For testing purposes, `index.html` was initially hardcoded. However, in the actual project, the `index.html` (and other code files) will be generated by the LLM (e.g., AI Pipe) in the `write_code_with_llm` function.

**Q23: Can the system handle multiple files generated by the LLM?**

**A23:** Yes, the system is designed to handle multiple files. The `push_files` function is configured to accept a list of file objects (each containing a name and content), allowing for flexible handling of LLM-generated code.

**Q24: What is the overall workflow for pushing files to GitHub?**

**A24:** The workflow for pushing files involves:

1.  **Getting SHA:** For Round 2 (updates), retrieve the SHA of the latest commit for the target file.
2.  **Preparing Payload:** Construct a JSON payload containing the file `name`, Base64 encoded `content`, a `commit message`, and (for updates) the file's `SHA`.
3.  **Making API Call:** Send a PUT request to the GitHub API endpoint for file content, including the payload and appropriate authentication headers.
4.  **Verifying Response:** Check the API response for a success status code (e.g., 200 for update, 201 for creation).
5.  **GitHub Pages:** If GitHub Pages is enabled, the changes will be reflected on the deployed page.

**Q25: What are the best practices for coding this project, considering development pace?**

**A25:** It's recommended to:

1.  **Observe first:** Watch the tutorial to understand the full process.
2.  **Code independently:** Implement the logic on your own after understanding the concepts.
3.  **Test iteratively:** Test each function (e.g., `create_repo`, `push_files`) independently as you develop it.
4.  **Use AI Pipe:** Leverage AI Pipe for LLM integration as recommended.
5.  **Utilize GitHub API:** Directly interact with the GitHub API for repository management.

**Q26: Is there an SDK for GitHub API in Python that would make integration easier?**

**A26:** Yes, Python has SDKs like `PyGithub` (often referred to as `github-sdk` or `github-api-python`) that can simplify interactions with the GitHub API, potentially offering a cleaner way to perform operations compared to raw HTTP requests. The instructor confirmed this is a viable option for those who prefer using an SDK.

**Q27: What is the recommended strategy for providing LLM prompts and handling context for code generation?**

**A27:** You will need to provide context to the LLM (e.g., AI Pipe) for code generation. If you have an existing repository, you might consider feeding its contents to the LLM. However, be mindful of token consumption, as feeding large amounts of existing code can be costly. The LLM needs sufficient context to understand what changes or new code it should generate.

---
